{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Install libraries"
      ],
      "metadata": {
        "id": "zWnJjQd3VMZM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmrCF3pOUvqN"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install 'huggingface_hub[tensorflow]'\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import libraries"
      ],
      "metadata": {
        "id": "y-RyRj2hVpbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from datasets import load_dataset,load_dataset_builder\n",
        "from transformers import AutoTokenizer, AutoConfig, TFAutoModelForPreTraining,AdamWeightDecay\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "5xR5Kq17Vscf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Declare config parameters"
      ],
      "metadata": {
        "id": "s0DOhjp7YO4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DRIVE_MOUNT_PATH=\"/content/drive/\"\n",
        "\n",
        "FULL_DATASET_PATH = DRIVE_MOUNT_PATH + \"MyDrive/colab/product_review_generator/datasets/full_dataset\"\n",
        "TRAIN_DATASET_PATH = DRIVE_MOUNT_PATH + \"MyDrive/colab/product_review_generator/datasets/train_dataset\"\n",
        "VAL_DATASET_PATH= DRIVE_MOUNT_PATH + \"MyDrive/colab/product_review_generator/datasets/val_dataset\"\n",
        "MODEL_SAVE_PATH=DRIVE_MOUNT_PATH + \"MyDrive/colab/product_review_generator/model/fine_tuned\"\n",
        "\n",
        "DATASET_NAME=\"amazon_us_reviews\"\n",
        "SUBSET_NAME=\"Apparel_v1_00\"\n",
        "COLUMNS_TO_REMOVE = ['marketplace', 'customer_id', 'review_id', 'product_id', 'product_parent',\n",
        "                     'product_category','helpful_votes', 'total_votes', 'vine','review_date']\n",
        "\n",
        "RECORDS_PER_LABEL = 50000\n",
        "\n",
        "VALIDATION_DATA_SPLIT = 0.2\n",
        "\n",
        "MODEL_NAME=\"distilgpt2\"\n",
        "\n",
        "HF_MODEL_LOCAL_PATH=\"/content/model_local\"\n",
        "HF_MODEL_ID=\"praveenseb/product_review_generator\"\n",
        "HF_TOKEN = \"<HF_TOKEN>\"\n",
        "\n",
        "SPECIAL_TOKENS  = { \"bos_token\": \"<|BOS|>\",\n",
        "                   \"eos_token\": \"<|EOS|>\",\n",
        "                   \"unk_token\": \"<|UNK|>\",\n",
        "                   \"pad_token\": \"<|PAD|>\",\n",
        "                   \"sep_token\": \"<|SEP|>\"}\n",
        "\n",
        "REVIEW_LEN_MIN = 10\n",
        "REVIEW_LEN_MAX = 100\n",
        "\n",
        "TOKEN_LEN_MAX  = 300\n",
        "\n",
        "RATING_DEF = {\n",
        "    1: 'Terrible',\n",
        "    2: 'Bad',\n",
        "    3: 'Acceptable ',\n",
        "    4: 'Good',\n",
        "    5: 'Excellent'\n",
        "}\n",
        "\n",
        "TRAIN_BATCH_SIZE=32\n",
        "\n",
        "EPOCHS=2"
      ],
      "metadata": {
        "id": "_oVNGWeAYPC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mount Google Drive in the runtime's VM"
      ],
      "metadata": {
        "id": "HcC8eJmB63dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(DRIVE_MOUNT_PATH, force_remount=True)"
      ],
      "metadata": {
        "id": "HCggWUrO6_NP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fetch dataset info"
      ],
      "metadata": {
        "id": "vj2oWIkoWD-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "amz_builder =  load_dataset_builder(DATASET_NAME,  SUBSET_NAME)\n",
        "print(\"Dataset features -\",amz_builder.info.features)\n",
        "print(\"Dataset splits -\",amz_builder.info.splits)"
      ],
      "metadata": {
        "id": "VPZIgVv_Wo9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the dataset and save to disk"
      ],
      "metadata": {
        "id": "FDG34zEhXrmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "amz_dataset = load_dataset(DATASET_NAME,  SUBSET_NAME)\n",
        "\n",
        "#Save to disk\n",
        "amz_dataset.save_to_disk(FULL_DATASET_PATH)"
      ],
      "metadata": {
        "id": "zxIQ2ypNXw9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load from disk if there is a saved version avaiable\n",
        "#amz_dataset=datasets.load_from_disk(FULL_DATASET_PATH)"
      ],
      "metadata": {
        "id": "Mb7KOmos0jf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total number of records - \",amz_dataset[\"train\"].num_rows)"
      ],
      "metadata": {
        "id": "RDKDZ-ak1Vxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pre-process the dataset"
      ],
      "metadata": {
        "id": "CEB3WdR_Zhpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_val_data(input_dataset):\n",
        "  #Remove columns that are not required\n",
        "  input_dataset = input_dataset[\"train\"].remove_columns(COLUMNS_TO_REMOVE)\n",
        "\n",
        "  #Filter on verified_purchase= 1 (True) and review_body word count. Suffle the filtered dataset\n",
        "  filtered_dataset = input_dataset.filter(lambda example: example[\"verified_purchase\"] == 1 \n",
        "                                   and len(example[\"review_body\"].split()) in range(REVIEW_LEN_MIN,REVIEW_LEN_MAX)).shuffle()\n",
        "  print(\"Record count after filtering on verified_purchase and review_body word count -\",filtered_dataset.num_rows)\n",
        "\n",
        "  #Pick equal number of records for ratings 1 to 5\n",
        "  for i in range(1,6):\n",
        "    temp_dict=filtered_dataset.filter(lambda example: example[\"star_rating\"] == i).shuffle()[:RECORDS_PER_LABEL]\n",
        "    temp_dataset = datasets.Dataset.from_dict(temp_dict)\n",
        "    if i==1:\n",
        "      processed_dataset = temp_dataset\n",
        "    else:\n",
        "      processed_dataset = datasets.concatenate_datasets([processed_dataset,temp_dataset])\n",
        "\n",
        "  print(\"Number of records in the processed dataset -\",processed_dataset.num_rows)\n",
        "  return processed_dataset.train_test_split(shuffle = True, test_size=VALIDATION_DATA_SPLIT)"
      ],
      "metadata": {
        "id": "2g-dYv-5XcjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create Training and Validation datasets"
      ],
      "metadata": {
        "id": "-lcYLlqPcaP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "amz_train_val_dataset = create_train_val_data(amz_dataset)\n",
        "\n",
        "train_dataset = amz_train_val_dataset[\"train\"]\n",
        "val_dataset = amz_train_val_dataset[\"test\"]\n",
        "\n",
        "#save the final train and test datasets to disk\n",
        "train_dataset.save_to_disk(TRAIN_DATASET_PATH)\n",
        "val_dataset.save_to_disk(VAL_DATASET_PATH)"
      ],
      "metadata": {
        "id": "51ZUULj-chDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load from disk if there is a saved version avaiable\n",
        "\n",
        "#train_dataset=datasets.load_from_disk(TRAIN_DATASET_PATH)\n",
        "#val_dataset=datasets.load_from_disk(VAL_DATASET_PATH)"
      ],
      "metadata": {
        "id": "OpCNHSn908gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of training records -\",train_dataset.num_rows)\n",
        "print(\"Number of validation records -\",val_dataset.num_rows)"
      ],
      "metadata": {
        "id": "lxxwIM3i1Oah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizer and helper functions"
      ],
      "metadata": {
        "id": "pk4tc4UafVDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.add_special_tokens(SPECIAL_TOKENS)"
      ],
      "metadata": {
        "id": "JMeTICLnfQqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get rid of &#34 and <br /> tags in the text\n",
        "def clean_string(text):\n",
        "    text = str.replace(text,\"&#34;\",\"\\\"\")\n",
        "    text = str.replace(text,\"<br />\",\"\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "sBtRkS845wYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    example['input_text'] = SPECIAL_TOKENS['bos_token']+ \\\n",
        "    example['product_title']+ \\\n",
        "    SPECIAL_TOKENS['sep_token']+ \\\n",
        "    RATING_DEF[(example['star_rating'])]+ \\\n",
        "    SPECIAL_TOKENS['sep_token']+ \\\n",
        "    clean_string(example['review_headline'])+ \\\n",
        "    SPECIAL_TOKENS['sep_token']+ \\\n",
        "    clean_string(example['review_body'])+ \\\n",
        "    SPECIAL_TOKENS['eos_token']\n",
        "    \n",
        "    tokens = tokenizer(example[\"input_text\"], padding=\"max_length\", truncation=True, max_length=TOKEN_LEN_MAX)\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "UXuyXVTk5wlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_token = train_dataset.map(tokenize_function)\n",
        "val_token = val_dataset.map(tokenize_function)"
      ],
      "metadata": {
        "id": "tDvBVkGQ3KoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define the model"
      ],
      "metadata": {
        "id": "gQLTukREx8Ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained(MODEL_NAME,\n",
        "                                    bos_token_id=tokenizer.bos_token_id,\n",
        "                                    eos_token_id=tokenizer.eos_token_id,\n",
        "                                    sep_token_id=tokenizer.sep_token_id,\n",
        "                                    pad_token_id=tokenizer.pad_token_id,\n",
        "                                    output_hidden_states=False)\n",
        "\n",
        "model = TFAutoModelForPreTraining.from_pretrained(MODEL_NAME, config=config)\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "qf5t-dIxx979"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.0002,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.95,\n",
        "    staircase=True)\n",
        "    \n",
        "optimizer = AdamWeightDecay(learning_rate=lr_schedule)"
      ],
      "metadata": {
        "id": "UoEyj54lzIht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=optimizer)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "oAkCjHwVzN7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine-tune the model"
      ],
      "metadata": {
        "id": "wjvQrNCdzWci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_tf_dataset=model.prepare_tf_dataset(\n",
        "    train_token,shuffle=True,batch_size=TRAIN_BATCH_SIZE)\n",
        "\n",
        "val_tf_dataset=model.prepare_tf_dataset(\n",
        "    val_token,shuffle=True,batch_size=TRAIN_BATCH_SIZE)"
      ],
      "metadata": {
        "id": "vqHXpqaMzbsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#HF Login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "tPgAW1OwO4SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hfhub_callback = PushToHubCallback(\n",
        "    output_dir=HF_MODEL_LOCAL_PATH,\n",
        "    tokenizer=tokenizer,\n",
        "    save_strategy = \"epoch\",\n",
        "    checkpoint = True,\n",
        "    hub_model_id=HF_MODEL_ID,\n",
        "    hub_token = HF_TOKEN\n",
        ")"
      ],
      "metadata": {
        "id": "xbmiDBRacYoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_tf_dataset, epochs=EPOCHS,callbacks=[hfhub_callback])"
      ],
      "metadata": {
        "id": "sqdcdX9X0L4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(MODEL_SAVE_PATH)\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "RZBgOyZpZAni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss = model.evaluate(val_tf_dataset)\n",
        "print(\"Validation loss is \",val_loss)"
      ],
      "metadata": {
        "id": "XAw4MtnaZsX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate review text with the fine-tuned model"
      ],
      "metadata": {
        "id": "DyuXc3LJ5Fq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title = \"Columbia Women's Benton Springs Full-Zip Fleece Jacket\"\n",
        "rating = 5\n",
        "review_title = \"Awesome Jacket!\"\n",
        "\n",
        "prompt = SPECIAL_TOKENS['bos_token'] + title + \\\n",
        "                SPECIAL_TOKENS['sep_token'] +  RATING_DEF[rating] + SPECIAL_TOKENS['sep_token'] + \\\n",
        "                 review_title + SPECIAL_TOKENS['sep_token']\n",
        "print(\"The input prompt is -\",prompt) \n",
        "         \n",
        "prompt_tokens = tf.expand_dims(tf.convert_to_tensor(tokenizer.encode(prompt)),0)"
      ],
      "metadata": {
        "id": "8M9q0Ue65KUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate 10 sample reviews\n",
        "generated_text = model.generate(prompt_tokens,                                   \n",
        "                                min_length=10, \n",
        "                                max_length=150,\n",
        "                                top_k=30,                                 \n",
        "                                top_p=0.7,        \n",
        "                                temperature=0.9,\n",
        "                                repetition_penalty=2.0,\n",
        "                                num_return_sequences=10,\n",
        "                                do_sample=True\n",
        "                                )"
      ],
      "metadata": {
        "id": "LoydRAq85pdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, text in enumerate(generated_text):\n",
        "    text = tokenizer.decode(text, skip_special_tokens=False)\n",
        "    review = text.split(\"<|SEP|>\")[3].split(\"<|EOS|>\")[0]\n",
        "    print(\"\\n\",i,review)"
      ],
      "metadata": {
        "id": "jdAE2UBB8fi2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
